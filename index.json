[{"authors":["admin"],"categories":null,"content":"I am a doctoral researcher at the Institute of Mathematics of the Technische Universität Berlin. My research focuses on topics in applied and computational mathematics, in particular inverse problems, signal- and image recovery, machine learning and deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1641574937,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jmaces.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a doctoral researcher at the Institute of Mathematics of the Technische Universität Berlin. My research focuses on topics in applied and computational mathematics, in particular inverse problems, signal- and image recovery, machine learning and deep learning.","tags":null,"title":"","type":"authors"},{"authors":["Theophil Trippe","Martin Genzel","Jan Macdonald","Maximilian März"],"categories":null,"content":"","date":1639554600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"5cece16c12e4103ff30c60c35cdd629e","permalink":"https://jmaces.github.io/talk/2021-tampere/","publishdate":"2022-01-07T17:37:11+01:00","relpermalink":"/talk/2021-tampere/","section":"talk","summary":"**Invited Talk:** Presentation of our 1st place winning Helsinki Deblur Challenge challenge.","tags":["Deep Learning","Deep Neural Networks","Inverse Problems","Challenge Submission","Defocus Deblurring"],"title":"Learning to Invert Defocus Blur: A Data-Driven Approach to the Helsinki Deblur Challenge","type":"talk"},{"authors":["Martin Genzel","Ingo Gühring","Jan Macdonald","Maximilian März"],"categories":null,"content":"","date":1639428300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"1eb2decbd87662975885a1441f51510f","permalink":"https://jmaces.github.io/talk/2021-neurips/","publishdate":"2022-01-07T17:20:47+01:00","relpermalink":"/talk/2021-neurips/","section":"talk","summary":"**Poster:** Presentation of a detailed [analysis](publication/genzel-near-exact-2021) of our 1st place winning AAPM DL-Sparse-View CT challenge [submission](publication/genzel-aapm-2021).","tags":["Deep Learning","Deep Neural Networks","Inverse Problems","Challenge Submission","CT Reconstruction"],"title":"Near-Exact Recovery for Sparse-View CT via Data-Driven Methods","type":"talk"},{"authors":["Jan Macdonald","Stephan Wäldchen"],"categories":null,"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"90f40e904744cde8e9d0e1fa47aa96eb","permalink":"https://jmaces.github.io/publication/macdonald-complete-2021/","publishdate":"2022-01-07T15:49:22.531957Z","relpermalink":"/publication/macdonald-complete-2021/","section":"publication","summary":"We give a complete characterisation of families of probability distributions that are invariant under the action of ReLU neural network layers. The need for such families arises during the training of Bayesian networks or the analysis of trained neural networks, e.g., in the context of uncertainty quantification (UQ) or explainable artificial intelligence (XAI). We prove that no invariant parametrised family of distributions can exist unless at least one of the following three restrictions holds: First, the network layers have a width of one, which is unreasonable for practical neural networks. Second, the probability measures in the family have finite support, which basically amounts to sampling distributions. Third, the parametrisation of the family is not locally Lipschitz continuous, which excludes all computationally feasible families. Finally, we show that these restrictions are individually necessary. For each of the three cases we can construct an invariant family exploiting exactly one of the restrictions but not the other two.","tags":["Deep Neural Networks","Probability Distributions"],"title":"A Complete Characterisation of ReLU-Invariant Distributions","type":"publication"},{"authors":["Jan Macdonald","Mathieu Besançon","Sebastian Pokutta"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"30e35e1361dd364306eb994c3b3dbc30","permalink":"https://jmaces.github.io/publication/macdonald-interpretable-2021/","publishdate":"2022-01-07T15:49:22.531287Z","relpermalink":"/publication/macdonald-interpretable-2021/","section":"publication","summary":"We study the effects of constrained optimization formulations and Frank-Wolfe algorithms for obtaining interpretable neural network predictions. Reformulating the Rate-Distortion Explanations (RDE) method for relevance attribution as a constrained optimization problem provides precise control over the sparsity of relevance maps. This enables a novel multi-rate as well as a relevance-ordering variant of RDE that both empirically outperform standard RDE in a well-established comparison test. We showcase several deterministic and stochastic variants of the Frank-Wolfe algorithm and their effectiveness for RDE.","tags":null,"title":"Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps and Relevance Orderings","type":"publication"},{"authors":["Martin Genzel","Ingo Gühring","Jan Macdonald","Maximilian März"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"19c2f8cf4bb07f3e305486ff5688aa94","permalink":"https://jmaces.github.io/publication/genzel-near-exact-2021/","publishdate":"2022-01-07T16:10:35.55163Z","relpermalink":"/publication/genzel-near-exact-2021/","section":"publication","summary":"This work presents an empirical study on the design and training of iterative neural networks for image reconstruction from tomographic measurements with unknown geometry. It is based on insights gained during our participation in the recent AAPM DL-Sparse-View CT challenge and a further analysis of our winning submission (team name: robust-and-stable) subsequent to the competition period. The goal of the challenge was to identify the state of the art in sparse-view CT with data-driven techniques, thereby addressing a fundamental research question: Can neural-network-based solvers produce near-perfect reconstructions for noise-free data? We answer this in the affirmative by demonstrating that an iterative end-to-end scheme enables the computation of near-perfect solutions on the test set. Remarkably, the fanbeam geometry of the used forward model is completely inferred through a data-driven geometric calibration step.","tags":["Deep Neural Networks","CT Reconstruction","Inverse Problems","Challenge Report"],"title":"Near-Exact Recovery for Sparse-View CT via Data-Driven Methods","type":"publication"},{"authors":["Luis Oala","Cosmas Heiß","Jan Macdonald","Maximilian März","Gitta Kutyniok","Wojciech Samek"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"e9a4957788b5ae32234a585c0d7488d4","permalink":"https://jmaces.github.io/publication/oala-interval-2021/","publishdate":"2021-09-19T14:53:28.745387Z","relpermalink":"/publication/oala-interval-2021/","section":"publication","summary":"**Purpose** The quantitative detection of failure modes is important for making deep neural networks reliable and usable at scale. We consider three examples for common failure modes in image reconstruction and demonstrate the potential of uncertainty quantification as a fine-grained alarm system. \n\n**Methods** We propose a deterministic, modular and lightweight approach called Interval Neural Network (INN) that produces fast and easy to interpret uncertainty scores for deep neural networks. Importantly, INNs can be constructed post hoc for already trained prediction networks. We compare it against state-of-the-art baseline methods (MCDROP, PROBOUT). \n\n**Results** We demonstrate on controlled, synthetic inverse problems the capacity of INNs to capture uncertainty due to noise as well as directional error information. On a real-world inverse problem with human CT scans, we can show that INNs produce uncertainty scores which improve the detection of all considered failure modes compared to the baseline methods. \n\n**Conclusion** Interval Neural Networks offer a promising tool to expose weaknesses of deep image reconstruction models and ultimately make them more reliable. The fact that they can be applied post hoc to equip already trained deep neural network models with uncertainty scores makes them particularly interesting for deployment.","tags":["Deep Neural Networks","CT Reconstruction","Uncertainty Quantification","Adversarial Examples","Inverse Problems"],"title":"Detecting Failure Modes in Image Reconstructions with Interval Neural Network Uncertainty","type":"publication"},{"authors":["Martin Genzel","Jan Macdonald","Maximilian März"],"categories":null,"content":"","date":1627482600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"04eb7124af6a44e61b6c788501bf9ac8","permalink":"https://jmaces.github.io/talk/2021-aapm/","publishdate":"2021-07-21T21:19:03+02:00","relpermalink":"/talk/2021-aapm/","section":"talk","summary":"**Invited Talk:** Presentation of our 1st place winning AAPM DL-Sparse-View CT challenge [submission](publication/genzel-aapm-2021).","tags":["Deep Learning","Deep Neural Networks","Inverse Problems","Challenge Submission","CT Reconstruction"],"title":"AAPM DL-Sparse-View CT Challenge Submission Report: Designing an Iterative Network for Fanbeam-CT with Unknown Geometry","type":"talk"},{"authors":["Martin Genzel","Jan Macdonald","Maximilian März"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"35a2bf54d72bca6a9427f6dc761d975a","permalink":"https://jmaces.github.io/publication/genzel-aapm-2021/","publishdate":"2021-06-04T10:16:25.200837Z","relpermalink":"/publication/genzel-aapm-2021/","section":"publication","summary":"This report is dedicated to a short motivation and description of our contribution to the AAPM DL-Sparse-View CT Challenge (team name: \"robust-and-stable\"). The task is to recover breast model phantom images from limited view fanbeam measurements using data-driven reconstruction techniques. The challenge is distinctive in the sense that participants are provided with a collection of ground truth images and their noiseless, subsampled sinograms (as well as the associated limited view filtered backprojection images), but not with the actual forward model. Therefore, our approach first estimates the fanbeam geometry in a data-driven geometric calibration step. In a subsequent two-step procedure, we design an iterative end-to-end network that enables the computation of near-exact solutions.","tags":["Deep Neural Networks","CT Reconstruction","Inverse Problems","Challenge Report"],"title":"AAPM DL-Sparse-View CT Challenge Submission Report: Designing an Iterative Network for Fanbeam-CT with Unknown Geometry","type":"publication"},{"authors":["Jan Macdonald","Luis Oala","Maximilian März","Wojciech Samek"],"categories":null,"content":"","date":1615293900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"7a5c96d79041b42ab0064c8f8c4c1a74","permalink":"https://jmaces.github.io/talk/2021-bvm/","publishdate":"2021-02-01T18:05:33+01:00","relpermalink":"/talk/2021-bvm/","section":"talk","summary":"**Contributed Talk:** Presentation of results from our [paper](publication/macdonald-interval-2021).","tags":["Deep Learning","Deep Neural Networks","Inverse Problems","Uncertainty Quantification","Adversarial Perturbations"],"title":"Interval Neural Networks as Instability Detectors for Image Reconstructions","type":"talk"},{"authors":["Jan Macdonald","Maximilian März","Luis Oala","Wojciech Samek"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"97795915887dcd78169e1f43e4a7000d","permalink":"https://jmaces.github.io/publication/macdonald-interval-2021/","publishdate":"2021-05-18T19:52:03.364909Z","relpermalink":"/publication/macdonald-interval-2021/","section":"publication","summary":"This work investigates the detection of instabilities that may occur when utilizing deep learning models for image reconstruction tasks. Although neural networks often empirically outperform traditional reconstruction methods, their usage for sensitive medical applications remains controversial. Indeed, in a recent series of works, it has been demonstrated that deep learning approaches are susceptible to various types of instabilities, caused for instance by adversarial noise or out-of-distribution features. It is argued that this phenomenon can be observed regardless of the underlying architecture and that there is no easy remedy. Based on this insight, the present work demonstrates, how uncertainty quantification methods can be employed as instability detectors. In particular, it is shown that the recently proposed Interval Neural Networks are highly effective in revealing instabilities of reconstructions. Such an ability is crucial to ensure a safe use of deep learning-based methods for medical image reconstruction.","tags":["Deep Neural Networks","CT Reconstruction","Uncertainty Quantification","Adversarial Examples","Inverse Problems"],"title":"Interval Neural Networks as Instability Detectors for Image Reconstructions","type":"publication"},{"authors":["Stephan Wäldchen","Jan Macdonald","Sascha Hauch","Gitta Kutyniok"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"0dd6496c5e0433d2f1fbda0af1ffd04a","permalink":"https://jmaces.github.io/publication/waldchen-computational-2021/","publishdate":"2021-02-01T21:41:42.910644Z","relpermalink":"/publication/waldchen-computational-2021/","section":"publication","summary":"For a $d$-ary Boolean function $\\Phi\\colon\\\\{0,1\\\\}^d\\to\\\\{0,1\\\\}$ and an assignment to its variables $\\mathbf{x}=(x\\_1, x\\_2, \\dots, x\\_d)$ we consider the problem of finding those subsets of the variables that are sufficient to determine the function value with a given probability $\\delta$. This is motivated by the task of interpreting predictions of binary classifiers described as Boolean circuits, which can be seen as special cases of neural networks. We show that the problem of deciding whether such subsets of relevant variables of limited size $k \\leq d$ exist is complete for the complexity class $\\mathsf{NP}^\\mathsf{PP}$ and thus, generally, unfeasible to solve. We then introduce a variant, in which it suffices to check whether a subset determines the function value with probability at least $\\delta$ or at most $\\delta-\\gamma$ for $00$, by a polynomial time algorithm unless $\\mathsf{P}=\\mathsf{NP}$. This holds even with the promise of a probability gap.","tags":["Complexity Theory","Explainable Neural Networks"],"title":"The Computational Complexity of Understanding Binary Classifier Decisions","type":"publication"},{"authors":["Martin Genzel","Jan Macdonald","Maximilian März"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"14b9daa4d07e74550b8926dc626f73b8","permalink":"https://jmaces.github.io/publication/genzel-solving-2020/","publishdate":"2021-02-01T20:57:01.612171Z","relpermalink":"/publication/genzel-solving-2020/","section":"publication","summary":"In the past five years, deep learning methods have become state-of-the-art in solving various inverse problems. Before such approaches can find application in safety-critical fields, a verification of their reliability appears mandatory. Recent works have pointed out instabilities of deep neural networks for several image reconstruction tasks. In analogy to adversarial attacks in classification, it was shown that slight distortions in the input domain may cause severe artifacts. The present article sheds new light on this concern, by conducting an extensive study of the robustness of deep-learning-based algorithms for solving underdetermined inverse problems. This covers compressed sensing with Gaussian measurements as well as image recovery from Fourier and Radon measurements, including a real-world scenario for magnetic resonance imaging (using the NYU-fastMRI dataset). Our main focus is on computing adversarial perturbations of the measurements that maximize the reconstruction error. A distinctive feature of our approach is the quantitative and qualitative comparison with total-variation minimization, which serves as a provably robust reference method. In contrast to previous findings, our results reveal that standard end-to-end network architectures are not only resilient against statistical noise, but also against adversarial perturbations. All considered networks are trained by common deep learning techniques, without sophisticated defense strategies.","tags":["Deep Neural Networks","MRI","Compressed Sensing","l1-Regularization","Iterative Reconstruction Algorithm","CT Reconstruction","Adversarial Examples","Inverse Problems"],"title":"Solving Inverse Problems With Deep Neural Networks - Robustness Included?","type":"publication"},{"authors":["Jan Macdonald","Stephan Wäldchen","Sascha Hauch","Gitta Kutyniok"],"categories":null,"content":"","date":1594944e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"287d54aa7503d277064ff38a964960c1","permalink":"https://jmaces.github.io/talk/2020-icml-xxai/","publishdate":"2020-08-17T22:02:01+02:00","relpermalink":"/talk/2020-icml-xxai/","section":"talk","summary":"**Poster:** Presentation of results based on our papers [A Rate-Distortion Framework for Explaining Neural Network Decisions](/publication/macdonald-rate-distortion-2019) and [The Computational Complexity of Understanding Network Decisions](/publication/waldchen-computational-2019).","tags":["Deep Learning","Deep Neural Networks","Explainable Neural Networks"],"title":"Explaining Neural Network Decisions Is Hard","type":"talk"},{"authors":["Jan Macdonald","Stephan Wäldchen","Sascha Hauch","Gitta Kutyniok"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"b1984c047c90720956f73fbfe433c66a","permalink":"https://jmaces.github.io/publication/macdonald-explaining-2020/","publishdate":"2022-01-07T15:49:22.532263Z","relpermalink":"/publication/macdonald-explaining-2020/","section":"publication","summary":"We connect the widespread idea of interpreting classiﬁer decisions to probabilistic prime implicants. A set of input features is deemed relevant for a classiﬁcation decision if the classiﬁer score remains nearly constant when randomising the remaining features. This introduces a rate-distortion trade-off between the set size and the deviation of the score. We explain how relevance maps can be interpreted as a greedy strategy to calculate the rate-distortion function. For neural networks we show that approximating this function even in a single point up to any non-trivial approximation factor is NP-hard. Thus, no algorithm will provably ﬁnd small relevant sets of input features even if they exist. Finally, as a numerical comparison we express a Boolean function, for which the prime implicant sets are known, as a neural network and investigate which relevance mapping methods are able to highlight them.","tags":["Deep Neural Networks","Explainable Neural Networks"],"title":"Explaining Neural Network Decisions Is Hard","type":"publication"},{"authors":["Luis Oala","Cosmas Heiß","Jan Macdonald","Maximilian März","Wojciech Samek","Gitta Kutyniok"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"eb24ea9caf8300fd1d77403833074fa8","permalink":"https://jmaces.github.io/publication/oala-interval-2020/","publishdate":"2020-08-13T21:11:34.703259Z","relpermalink":"/publication/oala-interval-2020/","section":"publication","summary":"We propose a fast, non-Bayesian method for producing uncertainty scores in the output of pre-trained deep neural networks (DNNs) using a data-driven interval propagating network. This interval neural network (INN) has interval valued parameters and propagates its input using interval arithmetic. The INN produces sensible lower and upper bounds encompassing the ground truth. We provide theoretical justification for the validity of these bounds. Furthermore, its asymmetric uncertainty scores offer additional, directional information beyond what Gaussian-based, symmetric variance estimation can provide. We find that noise in the data is adequately captured by the intervals produced with our method. In numerical experiments on an image reconstruction task, we demonstrate the practical utility of INNs as a proxy for the prediction error in comparison to two state-of-the-art uncertainty quantification methods. In summary, INNs produce fast, theoretically justified uncertainty scores for DNNs that are easy to interpret, come with added information and pose as improved error proxies - features that may prove useful in advancing the usability of DNNs especially in sensitive applications such as health care.","tags":["Deep Neural Networks","CT Reconstruction","Uncertainty Quantification","Inverse Problems"],"title":"Interval Neural Networks: Uncertainty Scores","type":"publication"},{"authors":["Jan Macdonald","Stephan Wäldchen","Sascha Hauch","Gitta Kutyniok"],"categories":null,"content":"","date":1571875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"98715d6502dc66fcfe79a725c0b886c3","permalink":"https://jmaces.github.io/talk/2019-gamm-cominds/","publishdate":"2020-08-17T22:00:58+02:00","relpermalink":"/talk/2019-gamm-cominds/","section":"talk","summary":"**Poster:** Presentation of results from our [paper](/publication/macdonald-rate-distortion-2019).","tags":["Deep Learning","Deep Neural Networks","Explainable Neural Networks"],"title":"A Rate-Distortion Framework for Explaining Neural Network Decisions","type":"talk"},{"authors":["Jan Macdonald","Stephan Wäldchen","Sascha Hauch","Gitta Kutyniok"],"categories":null,"content":"","date":1567987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"b43ec8d5a6666e3a95e4779c6ffe2edd","permalink":"https://jmaces.github.io/talk/2019-bzml/","publishdate":"2020-08-17T21:59:48+02:00","relpermalink":"/talk/2019-bzml/","section":"talk","summary":"**Poster:** Presentation of results from our [paper](/publication/macdonald-rate-distortion-2019).","tags":["Deep Learning","Deep Neural Networks","Explainable Neural Networks"],"title":"A Rate-Distortion Framework for Explaining Neural Network Decisions","type":"talk"},{"authors":["Jan Macdonald","Stephan Wäldchen","Sascha Hauch","Gitta Kutyniok"],"categories":null,"content":"","date":1563384600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"15ab4cad40978f0aa2918f8143c45c7c","permalink":"https://jmaces.github.io/talk/2019-iciam/","publishdate":"2020-08-17T21:59:26+02:00","relpermalink":"/talk/2019-iciam/","section":"talk","summary":"**Invited Talk:** Presentation of results from our [paper](/publication/macdonald-rate-distortion-2019).","tags":["Deep Learning","Deep Neural Networks","Explainable Neural Networks"],"title":"A Rate-Distortion Framework for Explaining Deep Neural Network Decisions","type":"talk"},{"authors":["Jan Macdonald","Stephan Wäldchen","Sascha Hauch","Gitta Kutyniok"],"categories":null,"content":"","date":1562112e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"8c03530d7c3acd8a9a8d6fd9adfe2aad","permalink":"https://jmaces.github.io/talk/2019-spars/","publishdate":"2020-08-17T21:59:21+02:00","relpermalink":"/talk/2019-spars/","section":"talk","summary":"**Poster:** Presentation of results from our [paper](/publication/macdonald-rate-distortion-2019).","tags":["Deep Learning","Deep Neural Networks","Explainable Neural Networks"],"title":"A Rate-Distortion Framework for Explaining Neural Network Decisions","type":"talk"},{"authors":["Jan Macdonald","Stephan Wäldchen","Sascha Hauch","Gitta Kutyniok"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"ef22235b1128580709e65d4b52800523","permalink":"https://jmaces.github.io/publication/macdonald-rate-distortion-2019/","publishdate":"2020-08-13T21:11:34.703609Z","relpermalink":"/publication/macdonald-rate-distortion-2019/","section":"publication","summary":"We formalise the widespread idea of interpreting neural network decisions as an explicit optimisation problem in a rate-distortion framework. A set of input features is deemed relevant for a classification decision if the expected classifier score remains nearly constant when randomising the remaining features. We discuss the computational complexity of finding small sets of relevant features and show that the problem is complete for $\\mathsf{NP}^\\mathsf{PP}$, an important class of computational problems frequently arising in AI tasks. Furthermore, we show that it even remains $\\mathsf{NP}$-hard to only approximate the optimal solution to within any non-trivial approximation factor. Finally, we consider a continuous problem relaxation and develop a heuristic solution strategy based on assumed density filtering for deep ReLU neural networks. We present numerical experiments for two image classification data sets where we outperform established methods in particular for sparse explanations of neural network decisions.","tags":["Deep Neural Networks","Explainable Neural Networks"],"title":"A Rate-Distortion Framework for Explaining Neural Network Decisions","type":"publication"},{"authors":["Jan Macdonald","Stephan Wäldchen","Sascha Hauch","Gitta Kutyniok"],"categories":null,"content":"","date":1554206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"5775ff07781140f1cbce1959c55958ee","permalink":"https://jmaces.github.io/talk/2019-gamm-msia/","publishdate":"2020-08-17T21:59:16+02:00","relpermalink":"/talk/2019-gamm-msia/","section":"talk","summary":"**Contributed Talk:** Presentation of results from our [paper](/publication/macdonald-rate-distortion-2019).","tags":["Deep Learning","Deep Neural Networks","Explainable Neural Networks"],"title":"A Rate-Distortion Framework for Explaining Deep Neural Network Decisions","type":"talk"},{"authors":["Dominik Alfke","Weston Baines","Jan Blechschmidt","Mauricio J. del Razo Sarmina","Amnon Drory","Dennis Elbrächter","Nando Farchmin","Matteo Gambara","Silke Glas","Philipp Grohs","Peter Hinz","Danijel Kivaranovic","Christian Kümmerle","Gitta Kutyniok","Sebastian Lunz","Jan Macdonald","Ryan Malthaner","Gregory Naisat","Ariel Neufeld","Philipp Christian Petersen","Rafael Reisenhofer","Jun-Da Sheng","Laura Thesing","Philipp Trunschke","Johannes von Lindheim","David Weber","Melanie Weber"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"56abb997a8f45e80f6d02e7bd7319be9","permalink":"https://jmaces.github.io/publication/alfke-oracle-2019/","publishdate":"2020-08-13T21:11:34.710235Z","relpermalink":"/publication/alfke-oracle-2019/","section":"publication","summary":"We present a novel technique based on deep learning and set theory which yields exceptional classification and prediction results. Having access to a sufficiently large amount of labeled training data, our methodology is capable of predicting the labels of the test data almost always even if the training data is entirely unrelated to the test data. In other words, we prove in a specific setting that as long as one has access to enough data points, the quality of the data is irrelevant.","tags":["Deep Neural Networks"],"title":"The Oracle of DLphi","type":"publication"},{"authors":["Jan Macdonald","Raffael Raisenhofer"],"categories":null,"content":"","date":1539648e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"1ff6575062a456a914b57a61aeaf2baa","permalink":"https://jmaces.github.io/talk/2018-oberwolfach/","publishdate":"2020-08-17T21:58:45+02:00","relpermalink":"/talk/2018-oberwolfach/","section":"talk","summary":"**Practical Session:** Introduction to Tensorflow and hands-on tutorial on approximating smooth functions with neural networks *(joint with Raffael Raisenhofer)*.","tags":["Deep Learning","Deep Neural Networks","Approximation Theory"],"title":"Practical Session on Approximations with (Deep) Neural Networks","type":"talk"},{"authors":["Jan Macdonald"],"categories":null,"content":"","date":1517648400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"8a9459d432480c31c7d7d79614095c2c","permalink":"https://jmaces.github.io/talk/2018-rrw/","publishdate":"2018-02-03T09:00:00Z","relpermalink":"/talk/2018-rrw/","section":"talk","summary":"**Contributed Talk:** An analysis of the generalization error for multi-class multinomial logistic regression classifiers.","tags":["Statistical Learning","Image Classification"],"title":"Image Classification Using Wavelet und Shearlet Based Scattering Transforms","type":"talk"},{"authors":["Jan Macdonald","Lars Ruthotto"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641574937,"objectID":"aaf3a22349bc7eb0793de016e6679b2e","permalink":"https://jmaces.github.io/publication/macdonald-improved-2018/","publishdate":"2020-08-13T21:11:34.702131Z","relpermalink":"/publication/macdonald-improved-2018/","section":"publication","summary":"We present an improved technique for susceptibility artifact correction in echo-planar imaging (EPI), a widely used ultra-fast magnetic resonance imaging (MRI) technique. Our method corrects geometric deformations and intensity modulations present in EPI images. We consider a tailored variational image registration problem incorporating a physical distortion model and aiming at minimizing the distance of two oppositely distorted images subject to invertibility constraints. We derive a novel face-staggered discretization of the variational problem that renders the discretized distance function and constraints separable. Motivated by the presence of a smoothness regularizer, which leads to global coupling, we apply the alternating direction method of multipliers (ADMM) to split the problem into simpler subproblems. We prove the convergence of ADMM for this non-convex optimization problem. We show the superiority of our scheme compared to two state-of-the-art methods both in terms of correction quality and time-to-solution for 13 high-resolution 3D imaging datasets.","tags":["MRI","ADMM","Non-convex Optimization","Inverse Problems","Echo-Planar MRI"],"title":"Improved Susceptibility Artifact Correction of Echo-Planar MRI using the Alternating Direction Method of Multipliers","type":"publication"}]