@unpublished{macdonald_interval_2020,
 abstract = {This work investigates the detection of instabilities that may occur when utilizing deep learning models for image reconstruction tasks. Although neural networks often empirically outperform traditional reconstruction methods, their usage for sensitive medical applications remains controversial. Indeed, in a recent series of works, it has been demonstrated that deep learning approaches are susceptible to various types of instabilities, caused for instance by adversarial noise or out-of-distribution features. It is argued that this phenomenon can be observed regardless of the underlying architecture and that there is no easy remedy. Based on this insight, the present work demonstrates on two use cases how uncertainty quantification methods can be employed as instability detectors. In particular, it is shown that the recently proposed Interval Neural Networks are highly effective in revealing instabilities of reconstructions. Such an ability is crucial to ensure a safe use of deep learning-based methods for medical image reconstruction.},
 author = {Macdonald, Jan and M{\"a}rz, Maximilian and Oala, Luis and Samek, Wojciech},
 keywords = {Deep Neural Networks, Uncertainty Quantification, Adversarial Examples},
 month = {March},
 note = {arXiv: 2003.13471},
 title = {Interval Neural Networks as Instability Detectors for Image Reconstructions},
 url = {http://arxiv.org/abs/2003.13471},
 year = {2020}
}
