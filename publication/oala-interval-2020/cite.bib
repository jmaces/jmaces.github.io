@unpublished{oala_interval_2020,
 abstract = {We propose a fast, non-Bayesian method for producing uncertainty scores in the output of pre-trained deep neural networks (DNNs) using a data-driven interval propagating network. This interval neural network (INN) has interval valued parameters and propagates its input using interval arithmetic. The INN produces sensible lower and upper bounds encompassing the ground truth. We provide theoretical justification for the validity of these bounds. Furthermore, its asymmetric uncertainty scores offer additional, directional information beyond what Gaussian-based, symmetric variance estimation can provide. We find that noise in the data is adequately captured by the intervals produced with our method. In numerical experiments on an image reconstruction task, we demonstrate the practical utility of INNs as a proxy for the prediction error in comparison to two state-of-the-art uncertainty quantification methods. In summary, INNs produce fast, theoretically justified uncertainty scores for DNNs that are easy to interpret, come with added information and pose as improved error proxies - features that may prove useful in advancing the usability of DNNs especially in sensitive applications such as health care.},
 author = {Oala, Luis and Hei{\ss}, Cosmas and Macdonald, Jan and M{\"a}rz, Maximilian and Samek, Wojciech and Kutyniok, Gitta},
 keywords = {Deep Neural Networks, Uncertainty Quantification},
 month = {March},
 note = {arXiv: 2003.11566},
 shorttitle = {Interval Neural Networks},
 title = {Interval Neural Networks: Uncertainty Scores},
 url = {http://arxiv.org/abs/2003.11566},
 year = {2020}
}
