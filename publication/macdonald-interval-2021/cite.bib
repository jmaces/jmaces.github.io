@inproceedings{macdonald_interval_2021,
 abstract = {This work investigates the detection of instabilities that may occur when utilizing deep learning models for image reconstruction tasks. Although neural networks often empirically outperform traditional reconstruction methods, their usage for sensitive medical applications remains controversial. Indeed, in a recent series of works, it has been demonstrated that deep learning approaches are susceptible to various types of instabilities, caused for instance by adversarial noise or out-ofdistribution features. It is argued that this phenomenon can be observed regardless of the underlying architecture and that there is no easy remedy. Based on this insight, the present work demonstrates, how uncertainty quantification methods can be employed as instability detectors. In particular, it is shown that the recently proposed Interval Neural Networks are highly effective in revealing instabilities of reconstructions. Such an ability is crucial to ensure a safe use of deep learning-based methods for medical image reconstruction.},
 address = {Wiesbaden},
 author = {Macdonald, Jan and März, Maximilian and Oala, Luis and Samek, Wojciech},
 booktitle = {Bildverarbeitung für die Medizin 2021},
 doi = {10.1007/978-3-658-33198-6_79},
 editor = {Palm, Christoph and Deserno, Thomas M. and Handels, Heinz and Maier, Andreas and Maier-Hein, Klaus and Tolxdorff, Thomas},
 file = {Macdonald et al. - 2021 - Interval Neural Networks as Instability Detectors .pdf:/home/jan/Documents/tubCloud/pdf-repo/Macdonald et al. - 2021 - Interval Neural Networks as Instability Detectors .pdf:application/pdf},
 isbn = {978-3-658-33198-6},
 keywords = {CT Reconstruction, Uncertainty Quantification, Adversarial Examples, Inverse Problems},
 language = {de},
 month = {February},
 pages = {324--329},
 publisher = {Springer Fachmedien},
 series = {Informatik aktuell},
 title = {Interval Neural Networks as Instability Detectors for Image Reconstructions},
 year = {2021}
}
